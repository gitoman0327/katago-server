# Example config for C++ (non-python) gtp bot

# SEE NOTES ABOUT PERFORMANCE AND MEMORY USAGE IN gtp_example.cfg

# Logs------------------------------------------------------------------------------------

# Where to output log?
logFile = gtp.log

# Controls the number of moves after the first move in a variation.
# analysisPVLen = 15

# Report winrates for chat and analysis as (BLACK|WHITE|SIDETOMOVE).
reportAnalysisWinratesAs = BLACK

# Rules------------------------------------------------------------------------------------

# Uncomment this to make it so that if the game seems to be a handicap game, assume that white gets +1 point per
# black handicap stone. Some Go servers like OGS will silently give white such points without including it in the komi.
# NOTE: If KataGo's heurisics about what games are handicap games do not work for your game data, or you wish to analyze
# positions whose initial positions are not the initial position of a game (such that it would be impossible for KataGo
# to determine from your input whether the game was originally handicap or not), you should probably leave this
# unspecified and simply specify the "true" komi in each request - i.e. the literal number of points to add to white's
# on-the-board area score.
# whiteBonusPerHandicapStone = 1

# Assume that if black makes many moves in a row right at the start of the game, then the game is a handicap game.
# This is necessary on some servers and for some GUIs and also when initializing from many SGF files, which may
# set up a handicap games using repeated GTP "play" commands for black rather than GTP "place_free_handicap" commands.
# However, it may also lead to incorrect undersanding of komi if whiteBonusPerHandicapStone = 1 and a server does NOT
# have such a practice.
# Defaults to true. Uncomment and set to false to disable this behavior.
# assumeMultipleStartingBlackMovesAreHandicap = false

# Search limits-----------------------------------------------------------------------------------

# By default, if NOT specified in an individual request, limit maximum number of root visits per search to this much
maxVisits = 1000
# If provided, cap search time at this many seconds
# maxTime = 60

# Number of threads to use in each search in parallel for a single position.
# (NOTE: Analysis engine can specify number of positions to be able to search in parallel via command line argument)
numSearchThreads = 1

# GPU Settings-------------------------------------------------------------------------------

# Maximum number of positions to send to GPU at once. Note that you will also need to increase numSearchThreads
# to make use of this, as every thread in KataGo is synchronous, so with 1 thread max batch will only be 1 anyways.
nnMaxBatchSize = 16
# Cache up to 2 ** this many neural net evaluations in case of transpositions in the tree.
nnCacheSizePowerOfTwo = 18
# Size of mutex pool for nnCache is 2 ** this
nnMutexPoolSizePowerOfTwo = 14
# Randomize board orientation when running neural net evals?
nnRandomize = true

# How many threads should there be to feed positions to the neural net?
# Server threads are indexed 0,1,...(n-1) for the purposes of the below GPU settings arguments
# that specify which threads should use which GPUs.
# NOTE: This parameter is probably ONLY useful if you have multiple GPUs, since each GPU will need a thread.
# If you're tuning single-GPU performance, use numSearchThreads instead.
numNNServerThreadsPerModel = 1

# CUDA GPU settings--------------------------------------
# These only apply when using CUDA as the backend for inference.
# (For GTP, we only ever have one model, when playing matches, we might have more than one, see match_example.cfg)

# Default behavior tries to guess the 'best' GPU or device
# You will want to uncomment and adjust one or more of these lines to take advantage of a multi-gpu machine
# cudaDeviceToUse = 0 #use device 0 for all server threads (numNNServerThreadsPerModel) unless otherwise specified per-model or per-thread-per-model
# cudaDeviceToUseModel0 = 3 #use device 3 for model 0 for all threads unless otherwise specified per-thread for this model
# cudaDeviceToUseModel1 = 2 #use device 2 for model 1 for all threads unless otherwise specified per-thread for this model
# cudaDeviceToUseModel0Thread0 = 3 #use device 3 for model 0, server thread 0
# cudaDeviceToUseModel0Thread1 = 2 #use device 2 for model 0, server thread 1

# Uncomment these on NVIDIA devices with FP16 tensor cores for probably a speedup, at the cost of introducing some precision loss in the nn calculation.
# cudaUseFP16 = true
# cudaUseNHWC = true

# OpenCL GPU settings--------------------------------------
# These only apply when using OpenCL as the backend for inference.
# (For GTP, we only ever have one model, when playing matches, we might have more than one, see match_example.cfg)

# Default behavior tries to guess the 'best' GPU or device
# You will want to uncomment and adjust one or more of these lines to take advantage of a multi-gpu machine
# openclDeviceToUse = 0 #use device 0 for all server threads (numNNServerThreadsPerModel) unless otherwise specified per-model or per-thread-per-model
# openclDeviceToUseModel0 = 3 #use device 3 for model 0 for all threads unless otherwise specified per-thread for this model
# openclDeviceToUseModel1 = 2 #use device 2 for model 1 for all threads unless otherwise specified per-thread for this model
# openclDeviceToUseModel0Thread0 = 3 #use device 3 for model 0, server thread 0
# openclDeviceToUseModel0Thread1 = 2 #use device 2 for model 0, server thread 1

# Uncomment to tune OpenCL for every board size separately, rather than only the largest possible size
# openclReTunePerBoardSize = true

# Search randomization------------------------------------------------------------------------------
# Note that multithreading can also introduce a significant amount of nondeterminism.
# Not all of these parameters are applicable to analysis, some are only used for actual play

# Temperature for the early game, randomize between chosen moves with this temperature
chosenMoveTemperatureEarly = 0.5
# Decay temperature for the early game by 0.5 every this many moves, scaled with board size.
chosenMoveTemperatureHalflife = 19
# At the end of search after the early game, randomize between chosen moves with this temperature
chosenMoveTemperature = 0.10
# Subtract this many visits from each move prior to applying chosenMoveTemperature
# (unless all moves have too few visits) to downweight unlikely moves
chosenMoveSubtract = 0
# The same as chosenMoveSubtract but only prunes moves that fall below the threshold, does not affect moves above
chosenMovePrune = 1

# Use dirichlet noise for the root node policy?
rootNoiseEnabled = false
# Dirichlet noise alpha is set to this divided by number of legal moves. 10.83 produces an alpha of 0.03 on an empty 19x19 board.
rootDirichletNoiseTotalConcentration = 10.83
# Proportion of root policy that is noise
rootDirichletNoiseWeight = 0.25

# Using LCB for move selection?
useLcbForSelection = true
# How many stdevs a move needs to be better than another for LCB selection
lcbStdevs = 5.0
# Only use LCB override when a move has this proportion of visits as the top move
minVisitPropForLCB = 0.15

# Internal params------------------------------------------------------------------------------

# Scales the utility of winning/losing
winLossUtilityFactor = 1.0
# Scales the utility for trying to maximize score
staticScoreUtilityFactor = 0.20
dynamicScoreUtilityFactor = 0.20
# Adjust dynamic score center this proportion of the way towards zero, capped at a reasonable amount.
dynamicScoreCenterZeroWeight = 0.20
# The utility of getting a "no result" due to triple ko or other long cycle in non-superko rulesets (-1 to 1)
noResultUtilityForWhite = 0.0
# The number of wins that a draw counts as, for white. (0 to 1)
drawEquivalentWinsForWhite = 0.5

# Exploration constant for mcts
cpuctExploration = 1.1
# FPU reduction constant for mcts
fpuReductionMax = 0.2
# Use parent average value for fpu base point instead of point value net estimate
fpuUseParentAverage = true
# Amount to apply a downweighting of children with very bad values relative to good ones
valueWeightExponent = 0.5
# Slight incentive for the bot to behave human-like with regard to passing at the end, filling the dame,
# not wasting time playing in its own territory, etc, and not play moves that are equivalent in terms of
# points but a bit more unfriendly to humans.
rootEndingBonusPoints = 0.5
# Make the bot prune useless moves that are just prolonging the game to avoid losing yet
rootPruneUselessMoves = true

# How big to make the mutex pool for search synchronization
mutexPoolSize = 2048
# How many virtual losses to add when a thread descends through a node
numVirtualLossesPerThread = 1
